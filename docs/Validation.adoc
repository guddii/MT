= Validation

This section serves to validate the <<Design and Implementation>> of the proposed system, thereby ensuring that it meets the quality standards set for both design and performance efficiency.
The <<Quality Model>> should reveal any shortcomings inherent to this approach.

== Design Quality Analysis

The design quality model is divided into two parts, which are to be analyzed in detail.
The first part concerns the general <<Component Cohesion>> and recommendations for <<Component Coupling>>.

=== RRP Analysis

The <<RRP>> that proposes that the granularity of reuse is identical to that of release is not applicable to the system design and subsystem arrangement.
From the perspective of the project, each module can be reused at any time, either as it is contained by a shared package or can be moved there.
This is because the project is arranged as a multi-package repository, as described in the <<Project Structure>>.
From the perspective of <<HTTP>> <<API,APIs>>, this is not a problem, as the logic behind the request handlers is delegated to packages within the multi-package repository.
However, currently, the shared packages relevant for central aspects of the system are only separated into `core` and `ui`.
The `core` package contains global functions that are needed in all types of packages and applications, while the `ui` contains the parts that are used in graphical user interfaces and generic view logic.
Nevertheless, the `core` package could be divided into multiple packages, as not all of its functions are necessary in all dependent packages.
This would prevent the unnecessary release of packages if they were released separately.

=== CPP Analysis

According to the <<CCP>>, each component must change for the same reasons and at the same time.
However, this is not feasible with the system design.
For example, if the <<RDF>> vocabulary changes, every subsystem of the proposed system is affected, including the Solid Provider, the <<DPC>> middleware, and the <<DPC>> application.
The Solid Provider must update the already persisted data structure if it is changed to an incompatible vocabulary.
The DPC middleware produces new data in the form of the changed vocabulary, and the DPC application must be able to interpret and display the new data format if it requires custom view logic.
Considering the related requirements of the _Single Responsibility Principle_, which states that each component should provide a service to a single actor, this appears to be a feasible solution.
This is a satisfactory concern for both the <<DPC>> application and the middleware, since actors are defined as a group of one or more users.
In this case, all Solid Provider users are grouped as an actor.
The <<DPC>> application serves only as a user interface for handling access logs.
The middleware is a proxy module that monitors traffic to the web server and creates the necessary resources if all conditions are met.
However, complex scenarios where a single proxy module is responsible for multiple changes would violate the above rule.

=== CRP Analysis

The <<CRP>> demands that the components of a system should not impose unnecessary dependencies on others.
The scenario is analogous to the one described in the <<RRP Analysis>>.
From a <<Project Structure>> perspective, there are potential areas for improvement, but there are no violations on a conceptual level.
However, when considering the <<HTTP>> <<API,APIs>>, there are relevant differences.
With regard to component orchestration, it is possible to utilise the concept illustrated in xref:Logical_Topology_B[xrefstyle=short] in place of that in xref:Logical_Topology_A[xrefstyle=short].
This would result in a reduction in the required storage containers, with the system becoming dependent on the client storage container alone.
If the drawback to the system design is tolerable, the dependency on the module storage container could be regarded as an unnecessary dependency.

=== ADP Analysis

The <<ADP>> requires that no cyclical dependencies should be part of the system's design.
However, this is a component coupling principle that is not satisfied with the proposed approach.
When a <<Forwarded Request>> is used, referred from the proxy as an actor, the proxy will request itself, meaning it has a dependency on itself and thereby a cyclic dependency.
Even when a proxy is not requesting itself and requests are forwarded using forwarded headers, as illustrated in xref:lst-request-with-forwarded-headers[xrefstyle=short], the server would still have a cyclic dependency.
This is because the CSS requires the proxy hostname to be the base URL.
Consequently, when Solid Provider verifies an agent from its own instance, it will still request itself through the proxy, which is then cyclic again.

.Request with Forwarded Headers
[source,httprequest,id="lst-request-with-forwarded-headers",reftext="Listing {counter:listing}"]
----
GET http://server.example.tld/client/profile/card#me
X-Forwarded-Host: proxy.example.tld
X-Forwarded-Proto: http
----

=== TDD Analysis

The <<TDD>> proposes that the structural composition of a system cannot be predetermined at its inception.
Instead, it is expected that this structure will naturally evolve as the system itself progresses.
This is, in fact, an inherent aspect of proposed system development.
It appears to be resolvable within the modular framework of a proxy, as illustrated in xref:Logical_Topology_A[xrefstyle=short] and xref:Logical_Topology_B[xrefstyle=short].
Indeed, new proxy modules, such as the middleware component of the <<DPC>>, are added as additional middlewares, all of which are processed one after another.

=== SDP Analysis

Each variant of the <<Component Orchestration>> fulfills the need <<SDP>>, which requires that every component relies on a stable component.
Given that the change of data does not affect the change frequency, the Solid Provider subsystem has the lowest change frequency.
Given the conceptual purpose of the proposed system, all changes, including those made to the <<Solid Protocol>>, are handled in the proxy subsystem.
Consequently, the changes are delegated to the proxy, which results in the higher change frequency being outsourced to it.
Consequently, as the proxy is dependent on the server, the stable dependency requirement is satisfied.
Any potential users of the public endpoint, such as the <<DPC>> application or any other <<HTTP>> client, are reliant on the stable proxy-server construct.
As they are not responsible for any relevant application logic, they are free to modify their functionality as often as necessary.

=== SAP Analysis

The <<SAP>> is a principle that aims to separate high-level policies from the other functionalities of a system.
If <<RDF>>, as a basis of <<Solid Ecosystem>>, is taken seriously, this is an inherent concept of it and thereby a satisfiable quality criterion without any additional design considerations.
This is the case, as the full information structure and thereby the application logic is encapsulated in <<RDF>> data, although the processing needs to be implemented.
The <<Solid Application Interoperability>> specification unifies a significant portion of this functionality.

== Performance Efficiency Analysis

When performing a performance efficiency analysis, it is necessary to know what tests were run, how many errors occurred in those tests, and how the proposed system affected response time and throughput.
This provides a comprehensive view of the system's capabilities and limitations, or to identify potential areas for improvement.

The tests that have been executed are TP1-B-1-1-1, TP1-B-1-1-30, TP1-B-1-30-1, TP1-B-1-30-30, TP1-B-30-1-1, TP1-B-30-1-30, TP1-B-30-30-1, TP1-B-30-30-30, TP1-N-1-1-1, TP1-N-1-1-30, TP1-N-1-30-1, TP1-N-1-30-30, TP1-N-30-1-1, TP1-N-30-1-30, TP1-N-30-30-1, TP1-N-30-30-30, TP1-C-1-1-1, TP1-C-1-1-30, TP1-C-1-30-1, TP1-C-1-30-30, TP1-C-30-1-1, TP1-C-30-1-30, TP1-C-30-30-1, and TP1-C-30-30-30.
The identifier relate to the <<Test Parameters>> described previously.
The following section will analyze these data sets.

xref:tbl-test-run-summary-errors[xrefstyle=short], xref:tbl-test-run-response-times-average[xrefstyle=short], and xref:tbl-test-run-throughput[xrefstyle=short] summarize of the test runs for TP1-`i`-`p`-`q`-`r`, as presented in xref:tbl-test-parameters-matrix[xrefstyle=short] and listed in the previous paragraph.
They provide an overview of how the system behaves at different loads and configurations.
The first column of the tables refers to the executed test plan, followed by the `i` value of that test in the next column and the `p`, `q`, and `r`  values in the same column through the next row.
Table headers that appear below these variables indicate the configuration of these variables.
The order of the `i`
column configuration is sorted according to the increasing complexity of the proposed system.

Test Run Error Summary is presented in xref:tbl-test-run-summary-errors[xrefstyle=short].
Its shows the percent of failed requests, returning a network status codefootnote:[https://developer.mozilla.org/en-US/docs/Web/HTTP/Status] greater or equals `400`.
Other requests are considered successful, in a network status code range `100`-`399`.

Upon examination of the values in the table, it appears that the complexity of the application, as defined by the <<Data Privacy Cockpit Parameters>> (`i`), is directly proportional to the number of failed requests.
When the proxy module is bypassed, there is only one test run that failed: TP1-B-1-30-30. Enabling the module without any registered client storages will result in an increase in the number of failed test runs to three.
The list of failed test runs, when considering registered client storages, is as follows: TP1-C-1-30-30, TP1-C-30-1-30, and TP1-C-30-1-30, and TP1-C-1-1-30. This results in a total of four erroneous requests.
A review of their configuration reveals that they have one parameter in common: the `r` value, which is `30` in every configuration.
This leads to the conclusion that concurrent or parallel processing is an issue in this implementation.

.Test Run Error Summary in Percent
[cols="1,2,2,2",id="tbl-test-run-summary-errors"]
|===
^.^h| TP1
3+^.^h| `i`

^.^h| `p`-`q`-`r`
>.^h| B
>.^h| N
>.^h| C

^.^h| 1-1-1
>.^| 0.00
>.^| 0.00
>.^| 0.00

^.^h| 1-1-30
>.^| 0.00
>.^| 0.00
>.^| 10.67

^.^h| 1-30-1
>.^| 0.00
>.^| 0.00
>.^| 0.00

^.^h| 1-30-30
>.^| 2.08
>.^| 0.83
>.^| 22.67

^.^h| 30-1-1
>.^| 0.00
>.^| 0.00
>.^| 0.00

^.^h| 30-1-30
>.^| 0.00
>.^| 1.14
>.^| 10.31

^.^h| 30-30-1
>.^| 0.00
>.^| 0.00
>.^| 0.00

^.^h| 30-30-30
>.^| 0.00
>.^| 2.94
>.^| 1.67
|===

xref:tbl-test-run-response-times-average[xrefstyle=short] presents the averaged response time in seconds.
This confirms the results presented in xref:tbl-test-run-summary-errors[xrefstyle=short].
The test cases with an increased r-value also exhibit the highest values in terms of response time.
The lowest value is `9.16` seconds for TP1-B-30-1-30, while the highest is `107.65` seconds for TP1-C-30-1-30. The lowest value, which bypasses DPC functionality, represents a considerable magnitude.
In accordance with citenp:[nielsen_usability_1993] observations, the threshold has nearly been reached with regard to the user's capacity to concentrate on the process.
In the case of the highest value, this threshold was exceeded by a factor of ten.

.Test Run Average Response Times in Seconds
[cols="1,2,2,2",id="tbl-test-run-response-times-average"]
|===
^.^h| TP1
3+^.^h| `i`

^.^h| `p`-`q`-`r`
>.^h| B
>.^h| N
>.^h| C

^.^h| 1-1-1
>.^| 0.51
>.^| 3.13
>.^| 7.97

^.^h| 1-1-30
>.^| 10.57
>.^| 20.75
>.^| 39.53

^.^h| 1-30-1
>.^| 0.69
>.^| 0.72
>.^| 1.89

^.^h| 1-30-30
>.^| 14.58
>.^| 26.17
>.^| 44.19

^.^h| 30-1-1
>.^| 0.39
>.^| 0.56
>.^| 0.88

^.^h| 30-1-30
>.^| 9.16
>.^| 19.79
>.^| 107.65

^.^h| 30-30-1
>.^| 0.70
>.^| 3.65
>.^| 4.21

^.^h| 30-30-30
>.^| 12.01
>.^| 67.70
>.^| 32.20
|===

The overall performance of the proposed system is quantified by the throughput measurements presented in xref:tbl-test-run-throughput[xrefstyle=short].
The values listed are in transactions per second.
As observed in the measurements shown in xref:tbl-test-run-summary-errors[xrefstyle=short] and xref:tbl-test-run-response-times-average[xrefstyle=short], the throughput drops significantly when the complexity of the system and the amount of processes in parallel increases.
In considering the aspects identified by IBM as influencing throughput, namely processing overhead in the software, the degree of parallelism supported by the software, and the types of transactions processed, it appears that these factors may be plausible causes of the issues that have been found.

.Test Run Throughput in Transactions per Second
[cols="1,2,2,2",id="tbl-test-run-throughput"]
|===
^.^h| TP1
3+^.^h| `i`

^.^h| `p`-`q`-`r`
>.^h| B
>.^h| N
>.^h| C

^.^h| 1-1-1
>.^| 1.92
>.^| 0.32
>.^| 0.13

^.^h| 1-1-30
>.^| 2.82
>.^| 1.44
>.^| 0.15

^.^h| 1-30-1
>.^| 1.43
>.^| 1.38
>.^| 0.10

^.^h| 1-30-30
>.^| 1.03
>.^| 0.74
>.^| 0.08

^.^h| 30-1-1
>.^| 2.50
>.^| 1.78
>.^| 1.13

^.^h| 30-1-30
>.^| 3.26
>.^| 0.85
>.^| 0.13

^.^h| 30-30-1
>.^| 1.41
>.^| 0.27
>.^| 0.16

^.^h| 30-30-30
>.^| 2.48
>.^| 0.25
>.^| 0.74
|===

