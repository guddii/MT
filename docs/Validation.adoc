= Validation

This section serves to validate the <<Design and Implementation>> of the proposed system, thereby ensuring that it meets the quality standards set for both design and performance efficiency.
The <<Quality Model>> should reveal any shortcomings inherent to this approach.

== Design Quality Analysis

The design quality model is divided into two parts, which are to be analyzed in detail.
The first part concerns the general <<Component Cohesion>> and recommendations for <<Component Coupling>>.

=== RRP Analysis

The <<RRP>> that proposes that the granularity of reuse is identical to that of release is not applicable to the system design and subsystem arrangement.
From the perspective of the project, each module can be reused at any time, either as it is contained by a shared package or can be moved there.
This is because the project is arranged as a multi-package repository, as described in the <<Project Structure>>.
From the perspective of <<HTTP>> <<API,APIs>>, this is not a problem, as the logic behind the request handlers is delegated to packages within the multi-package repository.
However, currently, the shared packages relevant for central aspects of the system are only separated into `core` and `ui`.
The `core` package contains global functions that are needed in all types of packages and applications, while the `ui` contains the parts that are used in graphical user interfaces and generic view logic.
Nevertheless, the `core` package could be divided into multiple packages, as not all of its functions are necessary in all dependent packages.
This would prevent the unnecessary release of packages if they were released separately.

=== CPP Analysis

According to the <<CCP>>, each component must change for the same reasons and at the same time.
However, this is not feasible with the system design.
For example, if the <<RDF>> vocabulary changes, every subsystem of the proposed system is affected, including the Solid Provider, the <<DPC>> middleware, and the <<DPC>> application.
The Solid Provider must update the already persisted data structure if it is changed to an incompatible vocabulary.
The DPC middleware produces new data in the form of the changed vocabulary, and the DPC application must be able to interpret and display the new data format if it requires custom view logic.
Considering the related requirements of the _Single Responsibility Principle_, which states that each component should provide a service to a single actor, this appears to be a feasible solution.
This is a satisfactory concern for both the <<DPC>> application and the middleware, since actors are defined as a group of one or more users.
In this case, all Solid Provider users are grouped as an actor.
The <<DPC>> application serves only as a user interface for handling access logs.
The middleware is a proxy module that monitors traffic to the web server and creates the necessary resources if all conditions are met.
However, complex scenarios where a single proxy module is responsible for multiple changes would violate the above rule.

=== CRP Analysis

The <<CRP>> demands that the components of a system should not impose unnecessary dependencies on others.
The scenario is analogous to the one described in the <<RRP Analysis>>.
From a <<Project Structure>> perspective, there are potential areas for improvement, but there are no violations on a conceptual level.
However, when considering the <<HTTP>> <<API,APIs>>, there are relevant differences.
With regard to component orchestration, it is possible to utilise the concept illustrated in xref:Logical_Topology_B[xrefstyle=short] in place of that in xref:Logical_Topology_A[xrefstyle=short].
This would result in a reduction in the required storage containers, with the system becoming dependent on the client storage container alone.
If the drawback to the system design is tolerable, the dependency on the module storage container could be regarded as an unnecessary dependency.

=== ADP Analysis

The <<ADP>> requires that no cyclical dependencies should be part of the system's design.
However, this is a component coupling principle that is not satisfied with the proposed approach.
When a <<Forwarded Request>> is used, referred from the proxy as an actor, the proxy will request itself, meaning it has a dependency on itself and thereby a cyclic dependency.
Even when a proxy is not requesting itself and requests are forwarded using forwarded headers, as illustrated in xref:lst-request-with-forwarded-headers[xrefstyle=short], the server would still have a cyclic dependency.
This is because the CSS requires the proxy hostname to be the base URL.
Consequently, when Solid Provider verifies an agent from its own instance, it will still request itself through the proxy, which is then cyclic again.

.Request with Forwarded Headers
[source,httprequest,id="lst-request-with-forwarded-headers",reftext="Listing {counter:listing}"]
----
GET http://server.example.tld/client/profile/card#me
X-Forwarded-Host: proxy.example.tld
X-Forwarded-Proto: http
----

=== TDD Analysis

The <<TDD>> proposes that the structural composition of a system cannot be predetermined at its inception.
Instead, it is expected that this structure will naturally evolve as the system itself progresses.
This is, in fact, an inherent aspect of proposed system development.
It appears to be resolvable within the modular framework of a proxy, as illustrated in xref:Logical_Topology_A[xrefstyle=short] and xref:Logical_Topology_B[xrefstyle=short].
Indeed, new proxy modules, such as the middleware component of the <<DPC>>, are added as additional middlewares, all of which are processed one after another.

=== SDP Analysis

Each variant of the <<Component Orchestration>> fulfills the need <<SDP>>, which requires that every component relies on a stable component.
Given that the change of data does not affect the change frequency, the Solid Provider subsystem has the lowest change frequency.
Given the conceptual purpose of the proposed system, all changes, including those made to the <<Solid Protocol>>, are handled in the proxy subsystem.
Consequently, the changes are delegated to the proxy, which results in the higher change frequency being outsourced to it.
Consequently, as the proxy is dependent on the server, the stable dependency requirement is satisfied.
Any potential users of the public endpoint, such as the <<DPC>> application or any other <<HTTP>> client, are reliant on the stable proxy-server construct.
As they are not responsible for any relevant application logic, they are free to modify their functionality as often as necessary.

=== SAP Analysis

The <<SAP>> is a principle that aims to separate high-level policies from the other functionalities of a system.
If <<RDF>>, as a basis of <<Solid Ecosystem>>, is taken seriously, this is an inherent concept of it and thereby a satisfiable quality criterion without any additional design considerations.
This is the case, as the full information structure and thereby the application logic is encapsulated in <<RDF>> data, although the processing needs to be implemented.
The <<Solid Application Interoperability>> specification unifies a significant portion of this functionality.

== Performance Efficiency Analysis

Due to several errors that occurred during the execution of the performance efficiency analysis tests, the tests were divided into several attempts.
In order to obtain reliable results, each attempt had different test parameters and outcomes.

=== Attempt 1 (Performance Efficiency Analysis)

The initial attempt was designed to provide an overview of a greater number of tests, with parameters that were not intended to create a critical load on the system.
The objective was to identify potential areas of heavy load and to produce detailed insight based on these initial results.

[horizontal]
Parameters:: All test plans were executed with the variable `i` assigned to each element of the set `{B, N, C}`.
The variables `p`, `q`, and `r` were selected from the set `{1, 10, 30}`.
Total Reports:: 82
Reports:: TP1.1-B-1-1-1, TP1.1-B-1-1-10, TP1.1-B-1-1-30, TP1.1-B-1-10-1, TP1.1-B-1-10-10, TP1.1-B-1-10-30, TP1.1-B-1-30-1, TP1.1-B-1-30-10, TP1.1-B-1-30-30, TP1.1-B-10-1-1, TP1.1-B-10-1-10, TP1.1-B-10-1-30, TP1.1-B-10-10-1, TP1.1-B-10-10-10, TP1.1-B-10-10-30, TP1.1-B-10-30-1, TP1.1-B-10-30-10, TP1.1-B-10-30-30, TP1.1-B-30-1-1, TP1.1-B-30-1-10, TP1.1-B-30-1-30, TP1.1-B-30-10-1, TP1.1-B-30-10-10, TP1.1-B-30-10-30, TP1.1-B-30-30-1, TP1.1-B-30-30-10, TP1.1-B-30-30-30, TP1.1-N-1-1-1, TP1.1-N-1-1-10, TP1.1-N-1-1-30, TP1.1-N-1-10-1, TP1.1-N-1-10-10, TP1.1-N-1-10-30, TP1.1-N-1-30-1, TP1.1-N-1-30-10, TP1.1-N-1-30-30, TP1.1-N-10-1-1, TP1.1-N-10-1-10, TP1.1-N-10-1-30, TP1.1-N-10-10-1, TP1.1-N-10-10-10, TP1.1-N-10-10-30, TP1.1-N-10-30-1, TP1.1-N-10-30-10, TP1.1-N-10-30-30, TP1.1-N-30-1-1, TP1.1-N-30-1-10, TP1.1-N-30-1-30, TP1.1-N-30-10-1, TP1.1-N-30-10-10, TP1.1-N-30-10-30, TP1.1-N-30-30-1, TP1.1-N-30-30-10, TP1.1-N-30-30-30, TP1.1-C-1-1-1, TP1.1-C-1-1-10, TP1.1-C-1-1-30, TP1.1-C-1-10-1, TP1.1-C-1-10-10, TP1.1-C-1-10-30, TP1.1-C-1-30-1, TP1.1-C-1-30-10, TP1.1-C-1-30-30, TP1.1-C-10-1-1, TP1.1-C-10-1-10, TP1.1-C-10-1-30, TP1.1-C-10-10-1, TP1.1-C-10-10-10, TP1.1-C-10-10-30, TP1.1-C-10-30-1, TP1.1-C-10-30-10, TP1.1-C-10-30-30, TP1.1-C-30-1-1, TP1.1-C-30-1-10, TP1.1-C-30-1-30footnote:[https://www.guddii.de/SEACT/TP1.1-C-30-1-30/], TP1.1-C-30-10-1, TP1.1-C-30-10-10, TP1.1-C-30-10-30, TP1.1-C-30-30-1, TP1.1-C-30-30-10, and TP1.1-C-30-30-30footnote:[https://www.guddii.de/SEACT/TP1.1-C-30-30-30/].
Outcome:: The tests were conducted over a period of approximately seven days, including the occurrence of application errors.
On restarting the application, the tests could be continued from that point onwards.
Upon analysis of the state of the application, it was found that the `.meta` resources in the tested storage resources were missing.
These resources, however, are conceptually relevant, as they are flagging a storage resource as such.
This is a crucial step in the <<DPC Middleware>> to continue with any kind of logging.
As the precise time of the resource deletion could not be determined, all tests with `i` in `{N, C}` are considered invalid, as they might not have executed the logging procedure.
This may also explain the occurrence of results that appear unreasonable, such as TP1.1-C-30-30-30, which has a lower average response time (32.20s) than TP1.1-30-1-30 (107.65s), despite the necessity of traversing a greater number of ShapeTrees (`q`).

Further analysis of the performance efficiency has been omitted due to the invalidity of the test reports that were created.

=== Attempt 2 (Performance Efficiency Analysis)

The second attempt was planed with the same intend as the initial attempt, with a smaller scope, that only tackles the edge cases and brings less reports to analyse.
The primary concern however was to get valid results and to overcome the error that has been found in the first attempt.

[horizontal]
Parameters:: All test plans were executed with the variable `i` assigned to each element of the set `{B, N, C}`.
The variables `p`, `q`, and `r` were selected from the set `{1, 30}`.
Total Reports:: 25
Reports:: TP1.2-B-1-1-1,
TP1.2-B-1-1-30, TP1.2-B-1-30-1, TP1.2-B-1-30-30, TP1.2-B-30-1-1, TP1.2-B-30-1-30, TP1.2-B-30-30-1, TP1.2-B-30-30-30, TP1.2-N-1-1-1, TP1.2-N-1-1-30, TP1.2-N-1-30-1, TP1.2-N-1-30-30, TP1.2-N-30-1-1, TP1.2-N-30-1-30, TP1.2-N-30-30-1, TP1.2-N-30-30-30, TP1.2-C-1-1-1, TP1.2-C-1-1-30, TP1.2-C-1-30-1, TP1.2-C-1-30-30, TP1.2-C-30-1-1, TP1.2-C-30-1-30, TP1.2-C-30-30-1, and TP1.2-C-30-30-30.
Outcome:: The tests were conducted over a period of approximately three days, including the occurrence of application errors.
It appeared that the application was failing again, resulting in invalid results.
The reason for this failure was the same as the error that occurred in attempt 1.

Further analysis of the performance efficiency has been omitted due to the invalidity of the test reports that were created.

=== Attempt 3 (Performance Efficiency Analysis)

The erroneous behavior observed in <<Attempt 1 (Performance Efficiency Analysis), Attempt 1>> was not accidental, as verified in <<Attempt 2 (Performance Efficiency Analysis), Attempt 2>>.
Consequently, the third attempt was conducted under the assumption that an error would occur at some point, resulting in the loss of relevant data.
To further investigate this error, individual tests were run to examine the specific edge cases that led to these critical errors.
In the event of any error in the console, all tests were terminated immediately.

[horizontal]
Parameters:: All test plans were executed with the variable `i` pinned to `C`, which represents the most exhaustive <<DPC>> configuration.
The variables `p`, `q`, and `r` were selected individually from the set `{1, 10, 30}`.
Total Reports:: 7
Reports:: TP1.3-1-1-30,
TP1.3-1-1-10, TP1.3-30-30-10, TP1.3-30-10-10, TP1.3-10-10-10, TP1.3-10-1-10, and TP1.3-1-10-10.
Outcome:: The only tests that completed without error were TP1.3-1-1-10. All other tests resulted in one of two erroneous situations.
TP1.3-1-1-30, TP1.3-10-1-10, and TP1.3-1-10-10 had errors, as demonstrated in xref:lst-err-perf-1[xrefstyle=short].
The second error, as demonstrated in xref:lst-err-perf-2[xrefstyle=short], was thrown in TP1.3-30-30-10, TP1.3-30-10-10, and TP1.3-10-10-10.

A detailed analysis reveals two errors that occur internally while processing requests.
The most significant differences relate to the storage number (`p`) and the number of ShapeTrees (`q`).
However, a strict behavior could not be determined.
It appears that test plans executed with lower values for `p` and/or `q` than those used in other tests within this attempt result in an error message indicating that a file for the locking system of the solid provider is requested that does not exist.
This is a unique function of the <<CSS>> as described in <<Third-Party Software>>.
The corresponding error message is shown in Listing 1.

.Server Console Error
[source,id="lst-err-perf-1",reftext="Listing {counter:listing}"]
----
Process is halting due to an uncaughtException with error ENOENT: no such file or directory, stat '/SEACT/apps/server/data/storage/.internal/locks/00169a735ca3f756b7e8d18151283856'
/SEACT/node_modules/.pnpm/@solid+community-server@7.0.4/node_modules/@solid/community-server/dist/util/locking/FileSystemResourceLocker.js:152
            throw err;
            ^
@seact/server:start:
[Error: ENOENT: no such file or directory, stat '/SEACT/apps/server/data/storage/.internal/locks/00169a735ca3f756b7e8d18151283856'] {
  errno: -2,
  code: 'ECOMPROMISED',
  syscall: 'stat',
  path: '/SEACT/apps/server/data/storage/.internal/locks/00169a735ca3f756b7e8d18151283856'
}

Node.js v22.1.0
ELIFECYCLE Command failed with exit code 1.
----

Tests conducted with p and/or q values that were higher than those of other tests resulted in a fetch exception when attempting to locate storage resources.
This aligns with the results observed in <<Attempt 1 (Performance Efficiency Analysis), Attempt 1>> and <<Attempt 1 (Performance Efficiency Analysis), 2>>.
An example of this error is shown in xref:lst-err-perf-2[xrefstyle=short].

.Proxy Console Error
[source,httprequest,id="lst-err-perf-2",reftext="Listing {counter:listing}"]
----
TypeError: fetch failed
    at node:internal/deps/undici/undici:12502:13
    at async findStorage (/SEACT/packages/core/dist/index.js:617:29)
    at async findDataRegistrationsInClaimContainer (/SEACT/packages/core/dist/index.js:726:19)
    at async createLog (/SEACT/apps/proxy/dist/index.js:303:47)
----


